<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Resume - Start Bootstrap Theme</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.15.3/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">
                <span class="d-block d-lg-none">Clarence Taylor</span>
                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="assets/img/JonBarron_circle.jpg" alt="..." /></span>
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav">
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#about">About</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#experience">Research</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#education">Service</a></li>
                   
                </ul>
            </div>
        </nav>

        <!-- Page Content-->
        <div class="container-fluid p-0">
            <!-- About-->
            <section class="resume-section" id="about">
                <div class="resume-section-content">
                    <h1 class="mb-0">
                        Jon 
                        <span class="text-primary">Barron</span>
                    </h1>
                    <div class="subheading mb-5">
                        Citizenship: United States <br>Publications: <a href=":http://jonbarron.info"> http://jonbarron.info</a>
                        <br>Email: <a href="jonbarron@gmail.com">jonbarron@gmail.com</a>
                    </div>
                    <p class="lead mb-5">I am a staff research scientist at Google Research, where I work on computer vision and machine learning.
                     <br>At Google I've worked on <a href="https://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html">Portrait Light</a>, <a href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://www.google.com/get/cardboard/jump/">Jump</a>, <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, and <a href="https://www.youtube.com/watch?v=JSnB06um5r4">Glass</a>. I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a> and funded by the <a href="http://www.nsfgrfp.org/">NSF GRFP</a>. I did my bachelors at the <a href="http://cs.toronto.edu">University of Toronto</a>.</p>
                    <div class="social-icons">
                        <a class="social-icon" href="mailto:jonbarron@gmail.com"><i class="fas fa-envelope"></i></a> <!-- email-->
                        <a class="social-icon" href="assets/data/JonBarron-CV.pdf"><i class="far fa-file"></i></a><!-- CV-->
                        <a class="social-icon" href="assets/data/JonBarron-bio.txt"><i class="fas fa-address-card"></i></a><!-- Bio-->
                        <a class="social-icon" href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ"><i class="fas fa-book-open"></i></a><!-- Scholar-->
                        <a class="social-icon" href="https://twitter.com/jon_barron"><i class="fab fa-twitter"></i></a><!-- Twitter-->
                        <a class="social-icon" href="https://github.com/jonbarron/"><i class="fab fa-github"></i></a><!-- Github-->
                    </div>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Research-->
            <section class="resume-section" id="experience">
                <div class="resume-section-content">
                    <h2 class="mb-5">Research</h2>
                    <!-- 1-->
                    <div  class="container-fluid">
                        <div class="row">
                            <div class="col-md-4">
                                <div>
                                    <div class="one">
                                      
                                      <img src="assets/img/hypernerf_before.jpg" width="160">
                                    </div>
                                  </div>
                            </div>
                            <div class="col-md-8">
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://hypernerf.github.io/">
                                      <papertitle>HyperNeRF: A Higher-Dimensional Representation
                      for Topologically Varying Neural Radiance Fields</papertitle>
                                    </a>
                                    <br>
                                                  <a href="https://keunhong.com">Keunhong Park</a>,
                                                  <a href="https://utkarshsinha.com">Utkarsh Sinha</a>, 
                                                  <a href="https://phogzone.com/">Peter Hedman</a>,
                                    <strong>Jonathan T. Barron</strong>,
                                                  <a href="http://sofienbouaziz.com">Sofien Bouaziz</a>, <br>
                                                  <a href="https://www.danbgoldman.com">Dan B Goldman</a>,
                                                  <a href="http://www.ricardomartinbrualla.com">Ricardo Martin-Brualla</a>, 
                                                  <a href="https://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a>
                                    <br>
                                    <em>arXiv</em>, 2021 
                                    <br>
                                    <a href="https://hypernerf.github.io/">project page</a>
                                    /
                                    <a href="https://arxiv.org/abs/2106.13228">arXiv</a>
                                    <p></p>
                                    <p>Applying ideas from level set methods to NeRF lets you represent scenes that deform and change shape.</p>
                                  </td>
                            </div>
                        </div><br>
                        <!-- 2-->
                        <div  class="container-fluid">
                            <div class="row">
                                <div class="col-md-4">
                                    <div>
                                        <div class="one">
                                          
                                          <img src="assets/img/nerfactor_before.png" width="160">
                                        </div>
                                      </div>
                                </div>
                                <div class="col-md-8">
                                    <td style="padding:20px;width:75%;vertical-align:middle">
                                        <a href="https://people.csail.mit.edu/xiuming/projects/nerfactor/">
                                            <papertitle>NeRFactor: Neural Factorization of Shape and Reflectance<br>
                              Under an Unknown Illumination</papertitle>
                                            </a>
                                            <br>
                                            <a href="https://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>,
                                            <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
                                            <a href="https://boyangdeng.com/">Boyang Deng</a>,<br>
                                            <a href="https://www.pauldebevec.com/">Paul Debevec</a>,
                                            <a href="http://billf.mit.edu/">William T. Freeman</a>,
                                                          <strong>Jonathan T. Barron</strong>,
                                            <br>
                                            <em>arXiv</em>, 2021 
                                            <br>
                                            <a href="https://people.csail.mit.edu/xiuming/projects/nerfactor/">project page</a>
                                            /
                                            <a href="https://arxiv.org/abs/2106.01970">arXiv</a>
                                            /
                                            <a href="https://www.youtube.com/watch?v=UUVSPJlwhPg">video</a>
                                            <p></p>
                                            <p>By placing priors on illumination and materials, we can recover NeRF-like models of the intrinsics of a scene from a single multi-image capture.</p>
                                          </td>
                                            </div>
                                </div>
                            </div>
                    </div><br>
            <!-- 3-->   
            <div  class="container-fluid">
                <div class="row">
                    <div class="col-md-4">
                        <div>
                            <div class="one">
                              
                              <img src="assets/img/mipnerf_ipe.png" width="160">
                            </div>
                          </div>
                    </div>
                    <div class="col-md-8">
                        <td style="padding:20px;width:75%;vertical-align:middle">
                            <a href="http://jonbarron.info/mipnerf">
                                <papertitle>Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields</papertitle>
                              </a>
                              <br>
                              <strong>Jonathan T. Barron</strong>,
                              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
                              <a href="http://matthewtancik.com/">Matthew Tancik</a>, <br>
                              <a href="https://phogzone.com/">Peter Hedman</a>,
                              <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>,
                              <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>
                              <br>
                              <em>arXiv</em>, 2021 
                              <br>
                              <a href="http://jonbarron.info/mipnerf">project page</a>
                              /
                              <a href="https://arxiv.org/abs/2103.13415">arXiv</a>
                              /
                              <a href="https://youtu.be/EpH175PY1A0">video</a>
                                            /
                              <a href="https://github.com/google/mipnerf">code</a>
                              <p></p>
                              <p>NeRF is aliased, but we can anti-alias it by casting cones and prefiltering the positional encoding function.</p>
                                </div>
                    </div>
                </div>
        <br> 
        <!-- 4-->   
        <div  class="container-fluid">
            <div class="row">
                <div class="col-md-4">
                    <div>
                        <div class="one">
                          
                          <img src="assets/img/nerfbake_160.png" width="160">
                        </div>
                      </div>
                </div>
                <div class="col-md-8">
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="http://nerf.live">
                            <papertitle>Baking Neural Radiance Fields for Real-Time View Synthesis</papertitle>
                            </a>
                            <br>
                            <a href="https://phogzone.com/">Peter Hedman</a>,
                            <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
                            <a href="https://bmild.github.io/">Ben Mildenhall</a>,
                            <strong>Jonathan T. Barron</strong>,
                            <a href="https://www.pauldebevec.com/">Paul Debevec</a>
                            <br>
                            <em>arXiv</em>, 2021 
                            <br>
                            <a href="http://nerf.live">project page</a>
                            /
                            <a href="https://arxiv.org/abs/2103.14645">arXiv</a>
                            /
                            <a href="https://www.youtube.com/watch?v=5jKry8n5YO8">video</a>
                            /
                            <a href="https://nerf.live/#demos">demo</a>
                            <p></p>
                            <p>Baking a trained NeRF into a sparse voxel grid of colors and features lets you render it in real-time in your browser.</p>
                            </div>
                </div>
            </div>
    <br>  
    <!-- 5-->   
    <div  class="container-fluid">
        <div class="row">
            <div class="col-md-4">
                <div>
                    <div class="one">
                      
                      <img src="assets/img/ibrnet_before.jpg" width="160">
                    </div>
                  </div>
            </div>
            <div class="col-md-8">
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://ibrnet.github.io/">
                        <papertitle>IBRNet: Learning Multi-View Image-Based Rendering</papertitle>
                      </a>
                      <br>
                      <a href="https://www.cs.cornell.edu/~qqw/">Qianqian Wang</a>,
                      <a href="https://www.linkedin.com/in/zhicheng-wang-96116897/">Zhicheng Wang</a>,
                      <a href="https://www.kylegenova.com/">Kyle Genova</a>,
                      <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
                      <a href="https://scholar.google.com/citations?user=Rh9T3EcAAAAJ&hl=en">Howard Zhou</a>, <br>
                      <strong>Jonathan T. Barron</strong>, 
                      <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>,
                      <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>, 
                      <a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>
                      <br>
                      <em>CVPR</em>, 2021
                      <br>
                      <a href="https://ibrnet.github.io/">project page</a> /
                      <a href="https://github.com/googleinterns/IBRNet">code</a> / 
                      <a href="https://arxiv.org/abs/2102.13090">arXiv</a>
                      <p></p>
                      <p>By learning how to pay attention to input images at render time, 
                          we can amortize inference for view synthesis and reduce error rates by 15%.</p>
                        </div>
            </div>
        </div>
<br>
<!-- 6-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/hotdog.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://pratulsrinivasan.github.io/nerv/">
                    <papertitle>NeRV: Neural Reflection and Visibility Fields for Relighting and View Synthesis</papertitle>
                  </a>
                  <br>
                  <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
                  <a href="https://boyangdeng.com/">Boyang Deng</a>,
                  <a href="https://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>, <br>
                  <a href="http://matthewtancik.com/">Matthew Tancik</a>,
                  <a href="https://bmild.github.io/">Ben Mildenhall</a>,
                  <strong>Jonathan T. Barron</strong>
                  <br>
                  <em>CVPR</em>, 2021
                  <br>
                  <a href="https://pratulsrinivasan.github.io/nerv/">project page</a> /
                  <a href="https://www.youtube.com/watch?v=4XyDdvhhjVo">video</a> /
                  <a href="https://arxiv.org/abs/2012.03927">arXiv</a>
                  <p></p>
                  <p>Using neural approximations of expensive visibility integrals lets you recover relightable NeRF-like models.</p>
                    </div>
        </div>
    </div>
<br>      
<!-- 7-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/notre.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://www.matthewtancik.com/learnit">
                    <papertitle>Learned Initializations for Optimizing Coordinate-Based Neural Representations</papertitle>
                  </a>
                  <br>
                  <a href="http://matthewtancik.com/">Matthew Tancik*</a>,
                  <a href="https://bmild.github.io/">Ben Mildenhall*</a>,
                  <a href="https://www.linkedin.com/in/terrance-wang/">Terrance Wang</a>,
                  <a href="https://www.linkedin.com/in/divi-schmidt-262044180/">Divi Schmidt</a>, <br>
                  <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>
                  <br>
                  <em>CVPR</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                  <br>
                  <a href="http://www.matthewtancik.com/learnit">project page</a> /
                  <a href="https://www.youtube.com/watch?v=A-r9itCzcyo">video</a> /
                  <a href="https://arxiv.org/abs/2012.02189">arXiv</a> 
                  <p></p>
                  <p>Using meta-learning to find weight initializations for coordinate-based MLPs allows them to converge faster and generalize better.</p>
    </div>
        </div>
    </div>
<br> 
<!-- 8-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/nerfw_before.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://nerf-w.github.io/">
                    <papertitle>NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections</papertitle>
                  </a>
                  <br>
                  <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla*</a>,
                  <a href="https://scholar.google.com/citations?user=g98QcZUAAAAJ&hl=en">Noha Radwan*</a>,
                  <a href="https://research.google/people/105804/">Mehdi S. M. Sajjadi*</a>, <br>
                  <strong>Jonathan T. Barron</strong>,
                  <a href="https://scholar.google.com/citations?user=FXNJRDoAAAAJ&hl=en">Alexey Dosovitskiy</a>,
                  <a href="http://www.stronglyconvex.com/about.html">Daniel Duckworth</a>
                  <br>
                  <em>CVPR</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                  <br>
                  <a href="https://nerf-w.github.io/">project page</a> /
                  <a href="https://arxiv.org/abs/2008.02268">arXiv</a> /
                  <a href="https://www.youtube.com/watch?v=mRAKVQj5LRA">video</a>
                  <p></p>
                  <p>Letting NeRF reason about occluders and appearance variation produces photorealistic view synthesis using only unstructured internet photos.</p>
                </div>
        </div>
    </div>
<br> <!-- 9-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/inerf_before.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://www.matthewtancik.com/learnit">
                    <papertitle>Learned Initializations for Optimizing Coordinate-Based Neural Representations</papertitle>
                    <a href="http://yenchenlin.me/inerf/">
                        <papertitle>iNeRF: Inverting Neural Radiance Fields for Pose Estimation</papertitle>
                      </a>
                      <br>
                      <a href="https://yenchenlin.me/">Lin Yen-Chen</a>, 
                      <a href="http://www.peteflorence.com/">Pete Florence</a>, 
                      <strong>Jonathan T. Barron</strong>,  <br>
                      <a href="https://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU">Alberto Rodriguez</a>,
                      <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>,
                      <a href="https://scholar.google.com/citations?user=_BPdgV0AAAAJ&hl=en">Tsung-Yi Lin</a>
                      <br>
                      <em>arXiv</em>, 2020  
                      <br>
                      <a href="http://yenchenlin.me/inerf/">project page</a> /
                      <a href="https://arxiv.org/abs/2012.05877">arXiv</a> /
                      <a href="https://www.youtube.com/watch?v=eQuCZaQN0tI">video</a>
                      <p></p>
                      <p>Given an image of an object and a NeRF of that object, you can estimate that object's pose.
                      </p></div>
        </div>
    </div>
<br> <!-- 10-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/nerd_160.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://markboss.me">Mark Boss</a>, 
              <a href="https://uni-tuebingen.de/en/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/raphael-braun/">Raphael Braun</a>,
              <a href="https://varunjampani.github.io">Varun Jampani</a>,
              <strong>Jonathan T. Barron</strong>,
              <a href="http://people.csail.mit.edu/celiu/">Ce Liu</a>,
              <a href="https://uni-tuebingen.de/en/faculties/faculty-of-science/departments/computer-science/lehrstuehle/computergrafik/computer-graphics/staff/prof-dr-ing-hendrik-lensch/">Hendrik P. A. Lensch</a>
              <br>
              <em>arXiv</em>, 2020
              <br>
              <a href="https://markboss.me/publication/2021-nerd/">project page</a> /
              <a href="https://www.youtube.com/watch?v=JL-qMTXw9VU">video</a> /
              <a href="https://github.com/cgtuebingen/NeRD-Neural-Reflectance-Decomposition">code</a> /
              <a href="https://arxiv.org/abs/2012.03918">arXiv</a>
              <p></p>
              <p>
              A NeRF-like model that can decompose (and mesh) objects with non-Lambertian reflectances, complex geometry, and unknown illumination.
              </p></div>
        </div>
    </div>
<br> <!-- 11-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/nerfie_before.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://nerfies.github.io/">
                    <papertitle>Nerfies: Deformable Neural Radiance Fields</papertitle>
                  </a>
                  <br>
                  
                  <a href="https://keunhong.com">Keunhong Park</a>,
                  <a href="https://utkarshsinha.com">Utkarsh Sinha</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="http://sofienbouaziz.com">Sofien Bouaziz</a>, <br>
                  <a href="https://www.danbgoldman.com">Dan B Goldman</a>,
                  <a href="https://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a>,
                  <a href="http://www.ricardomartinbrualla.com">Ricardo-Martin Brualla</a>
                  <br>
                  <em>arXiv</em>, 2020  
                  <br>
                  <a href="https://nerfies.github.io/">project page</a> /
                  <a href="https://arxiv.org/abs/2011.12948">arXiv</a> /
                  <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA">video</a>
                  <p></p>
                  <p>Building deformation fields into NeRF lets you capture non-rigid subjects, like people.
                  </p></div>
        </div>
    </div>
<br> <!-- 12-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/flare_before.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2011.12485">
                    <papertitle>Single-Image Lens Flare Removal</papertitle>
                  </a>
                  <br>
                  <a href="http://yicheng.rice.edu/">Yicheng Wu</a>,
                  <a href="https://scholar.google.com/citations?user=BxqV_RsAAAAJ">Qiurui He</a>,
                  <a href="https://people.csail.mit.edu/tfxue/">Tianfan Xue</a>,
                  <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>, <br>
                  <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>,
                  <a href="https://computationalimaging.rice.edu/team/ashok-veeraraghavan/">Ashok Veeraraghavan</a>,
                  <strong>Jonathan T. Barron</strong>
                  <br>
                  <em>arXiv</em>, 2020  
                  <br>
                  <a href="https://yichengwu.github.io/flare-removal/">project page</a>  / 
                  <a href="https://arxiv.org/abs/2011.12485">arXiv</a> 
                  <p></p>
                  <p>
                    Simulating the optics of a camera's lens lets you train a model that removes lens flare from a single image.
                  </p></div>
        </div>
    </div>
<br> <!-- 13-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/c5_after.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2011.11890">
                    <papertitle>Cross-Camera Convolutional Color Constancy</papertitle>
                  </a>
                  <br>
                  <a href="https://sites.google.com/corp/view/mafifi">Mahmoud Afifi</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="http://www.chloelegendre.com/">Chloe LeGendre</a>,
                  <a href="https://research.google/people/105312/">Yun-Ta Tsai</a>,
                  <a href="https://www.linkedin.com/in/fbleibel/">Francois Bleibel</a>
                  <br>
                  <em>arXiv</em>, 2020  
                  <br>
                  <p></p>
                  <p>
                    With some extra (unlabeled) test-set images, you can build a hypernetwork that calibrates itself at test time to previously-unseen cameras.
                  </p></div>
        </div>
    </div>
<br> <!-- 14-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                    <img src="assets/img/lssr_before.jpg" width="160">
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://cseweb.ucsd.edu/~viscomp/projects/SIGA20LightstageSuperres/">
                    <papertitle>Light Stage Super-Resolution: Continuous High-Frequency Relighting</papertitle>
                  </a>
                  <br>
                  <a href="http://kevinkingo.com/">Tiancheng Sun</a>,
                  <a href="https://cseweb.ucsd.edu/~zex014/">Zexiang Xu</a>
                  <a href="http://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>,
                  <a href="http://www.seanfanello.it/">Sean Fanello</a>,
                  <a href="https://scholar.google.com/citations?user=5D0_pjcAAAAJ&hl=en">Christoph Rhemann</a>,
                  <a href="https://www.pauldebevec.com/">Paul Debevec</a>,
                  <a href="https://research.google/people/105312/">Yun-Ta Tsai</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="https://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>
                  <br>
                  <em>SIGGRAPH Asia</em>, 2020  
                  <br>
                  <a href="http://cseweb.ucsd.edu/~viscomp/projects/SIGA20LightstageSuperres/">project page</a> / 
                  <a href="https://arxiv.org/abs/2010.08888">arXiv</a>
                  <p></p>
                  <p>
                    Scans for light stages are inherently aliased, but we can use learning to super-resolve them.
                  </p></div>
        </div>
    </div>
<br> <!-- 15-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/dualrefl_before.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://sniklaus.com/dualref">
                    <papertitle>Learned Dual-View Reflection Removal</papertitle>
                  </a>
                  <br>
                  <a href="http://sniklaus.com/welcome">Simon Niklaus</a>,
                  <a href="https://people.eecs.berkeley.edu/~cecilia77/">Xuaner (Cecilia) Zhang</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="http://nealwadhwa.com">Neal Wadhwa</a>,
                  <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>,
                  <a href="http://web.cecs.pdx.edu/~fliu/">Feng Liu</a>,
                  <a href="https://people.csail.mit.edu/tfxue/">Tianfan Xue</a>,
                  <br>
                  <em>WACV</em>, 2021
                  <br>
                  <a href="http://sniklaus.com/dualref">project page</a> /
                  <a href="https://arxiv.org/abs/2010.00702">arXiv</a>
                  <p></p>
                  <p>
                    Reflections and the things behind them often exhibit parallax, and this lets you remove reflections from stereo pairs.
                  </p></div>
        </div>
    </div>
<br> <!-- 16-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/nlt_before.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://nlt.csail.mit.edu/">
                    <papertitle>Neural Light Transport for Relighting and View Synthesis</papertitle>
                  </a>
                  <br>
                  <a href="http://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>,
                  <a href="http://www.seanfanello.it/">Sean Fanello</a>,
                  <a href="https://research.google/people/105312/">Yun-Ta Tsai</a>,
                  <a href="http://kevinkingo.com/">Tiancheng Sun</a>,
                  <a href="https://people.csail.mit.edu/tfxue/">Tianfan Xue</a>,
                  <a href="https://research.google/people/106687/">Rohit Pandey</a>,
                  <a href="https://www.dtic.ua.es/~sorts/">Sergio Orts-Escolano</a>,
                  <a href="https://dl.acm.org/profile/99659224296">Philip Davidson</a>,
                  <a href="https://scholar.google.com/citations?user=5D0_pjcAAAAJ&hl=en">Christoph Rhemann</a>,
                  <a href="http://www.pauldebevec.com/">Paul Debevec</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
                  <a href="http://billf.mit.edu/">William T. Freeman</a>
                  <br>
                  <em>ACM TOG</em>, 2021
                  <br>
                  <a href="http://nlt.csail.mit.edu/">project page</a> /
                  <a href="https://arxiv.org/abs/2008.03806">arXiv</a> /
                  <a href="https://www.youtube.com/watch?v=OGEnCWZihHE">video</a>
                  <p></p>
                  <p>Embedding a convnet within a predefined texture atlas enables simultaneous view synthesis and relighting.</p>
                </div>
        </div>
    </div>
<br> <!-- 17-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/lion_ff.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://bmild.github.io/fourfeat/index.html">
                    <papertitle>Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains</papertitle>
                  </a>
                  <br>
                  <a href="http://matthewtancik.com/">Matthew Tancik*</a>,
                  <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan*</a>,
                  <a href="https://bmild.github.io/">Ben Mildenhall*</a>,
                  <a href="https://people.eecs.berkeley.edu/~sfk/">Sara Fridovich-Keil</a>,
                  <a href="https://www.linkedin.com/in/nithinraghavan">Nithin Raghavan</a>,
                  <a href="https://scholar.google.com/citations?user=lvA86MYAAAAJ&hl=en">Utkarsh Singhal</a>,
                  <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>
                  <br>
                  <em>NeurIPS</em>, 2020 &nbsp <font color=#FF8080><strong>(Spotlight)</strong></font>
                  <br>
                  <a href="https://bmild.github.io/fourfeat/">project page</a> /
                  video: <a href="https://www.youtube.com/watch?v=nVA6K6Sn2S4">3 min</a>, <a href="https://www.youtube.com/watch?v=iKyIJ_EtSkw">10 min</a> /
                  <a href="https://arxiv.org/abs/2006.10739">arXiv</a> /
                  <a href="https://github.com/tancik/fourier-feature-networks">code</a>
                  <p></p>
                  <p>Composing neural networks with a simple Fourier feature mapping allows them to learn detailed high-frequency functions.</p>
                </div>
        </div>
    </div>
<br> <!-- 18-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/thresh_before.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2007.07350">
                    <papertitle>A Generalization of Otsu's Method and Minimum Error Thresholding</papertitle>
                  </a>
                  <br>
                  <strong>Jonathan T. Barron</strong>
                  <br>
                  <em>ECCV</em>, 2020 &nbsp <font color=#FF8080><strong>(Spotlight)</strong></font>
                  <br>
                  <a href="https://github.com/jonbarron/hist_thresh">code</a> / 
                  <a href="https://www.youtube.com/watch?v=rHtQQlQo1Q4">video</a> / 
                  <a href="data/BarronECCV2020.bib">bibtex</a>
                  <br>
                  <p></p>
                  <p>
                  A simple and fast Bayesian algorithm that can be written in ~10 lines of code outperforms or matches giant CNNs on image binarization, and unifies three classic thresholding algorithms.
                  </p></div>
        </div>
    </div>
<br> <!-- 19-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/uflow_after.png" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2006.04902">
                    <papertitle>What Matters in Unsupervised Optical Flow</papertitle>
                  </a>
                  <br>
                  <a href="http://ricojonschkowski.com/">Rico Jonschkowski</a>,
                  <a href="https://www.linkedin.com/in/austin-charles-stone-1ba33b138/">Austin Stone</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="https://research.google/people/ArielGordon/">Ariel Gordon</a>,
                  <a href="https://www.linkedin.com/in/kurt-konolige/">Kurt Konolige</a>,
                  <a href="https://research.google/people/AneliaAngelova/">Anelia Angelova</a>
                  <br>
                  <em>ECCV</em>, 2020 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                  <br>
                  <a href="https://github.com/google-research/google-research/tree/master/uflow">code</a>
                  <br>
                  <p></p>
                  <p>
                  Extensive experimentation yields a simple optical flow technique that is trained on only unlabeled videos, but still works as well as supervised techniques.
                  </p></div>
        </div>
    </div>
<br> <!-- 20-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/vase_still.png" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://www.matthewtancik.com/nerf">
                    <papertitle>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</papertitle>
                  </a>
                  <br>
                  <a href="https://bmild.github.io/">Ben Mildenhall*</a>,
                  <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan*</a>,
                  <a href="http://matthewtancik.com/">Matthew Tancik*</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
                  <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>
                  <br>
                  <em>ECCV</em>, 2020 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Honorable Mention, CACM Research Highlight)</strong></font>
                  <br>
                  <a href="http://www.matthewtancik.com/nerf">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2003.08934">arXiv</a>
                  /
                  <a href="https://www.youtube.com/watch?v=LRAqeM8EjOo&t">talk video</a>
                  /
                  <a href="https://www.youtube.com/watch?v=JuH79E8rdKc">supp video</a>
                  /
                  <a href="https://github.com/bmild/nerf">code</a>
                  /
                  <a href="http://cseweb.ucsd.edu/~ravir/cacm.pdf">CACM</a>
                  <p></p>
                  <p>
                  Training a tiny non-convolutional neural network to reproduce a scene using volume rendering achieves photorealistic view synthesis.</p>
                </div>
        </div>
    </div>
<br> <!-- 21-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/porshadmanip_before.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2005.08925">
                    <papertitle>Portrait Shadow Manipulation</papertitle>
                  </a>
                  <br>
                  <a href="https://people.eecs.berkeley.edu/~cecilia77/">Xuaner (Cecilia) Zhang</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="https://ai.google/research/people/105312/">Yun-Ta Tsai</a>,
                  <a href="https://www.linkedin.com/in/rohit-pandey-bab10b7a/">Rohit Pandey</a>,
                  <a href="http://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>,
                  <a href="http://graphics.stanford.edu/~renng/">Ren Ng</a>,
                  <a href="http://graphics.stanford.edu/~dejacobs/">David E. Jacobs</a>
                  <br>
                  <em>SIGGRAPH</em>, 2020  
                  <br>
                  <a href="https://people.eecs.berkeley.edu/~cecilia77/project-pages/portrait">project page</a> / 
                  <a href="https://www.youtube.com/watch?v=M_qYTXhzyac">video</a>
                  <p></p>
                  <p>Networks can be trained to remove shadows cast on human faces and to soften harsh lighting.</p>
                </div>
        </div>
    </div>
<br> <!-- 22-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/learnaf_after.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2004.12260">
                    <papertitle>Learning to Autofocus</papertitle>
                  </a>
                  <br>
                  <a href="">Charles Herrmann</a>,
                  <a href="">Richard Strong Bowen</a>,
                  <a href="http://nealwadhwa.com">Neal Wadhwa</a>,
                  <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>,
                  <a href="https://scholar.google.com/citations?user=BxqV_RsAAAAJ">Qiurui He</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="http://www.cs.cornell.edu/~rdz/index.htm">Ramin Zabih</a>
                  <br>
                  <em>CVPR</em>, 2020  
                  <br>
                                <a href="https://learntoautofocus-google.github.io/">project page</a>
                                /
                  <a href="https://arxiv.org/abs/2004.12260">arXiv</a>
                  <p></p>
                  <p>Machine learning can be used to train cameras to autofocus (which is not the same problem as "depth from defocus").</p>
                </div>
        </div>
    </div>
<br> <!-- 23-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/rings.png" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://pratulsrinivasan.github.io/lighthouse/">
                    <papertitle>Lighthouse: Predicting Lighting Volumes for Spatially-Coherent Illumination</papertitle>
                  </a>
                  <br>
                  <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan*</a>,
                  <a href="https://bmild.github.io/">Ben Mildenhall*</a>,
                  <a href="http://matthewtancik.com/">Matthew Tancik</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="https://research.google/people/RichardTucker/">Richard Tucker</a>,
                  <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>
                  <br>
            <em>CVPR</em>, 2020  
                  <br>
                  <a href="https://pratulsrinivasan.github.io/lighthouse/">project page</a>
            /
                  <a href="https://github.com/pratulsrinivasan/lighthouse">code</a>
            /
                  <a href="https://arxiv.org/abs/2003.08367">arXiv</a>
            /
                  <a href="https://www.youtube.com/watch?v=KsiZpUFPqIU">video</a>
                  <p></p>
                  <p>We predict a volume from an input stereo pair that can be used to calculate incident lighting at any 3D point within a scene.</p>
               </div>
        </div>
    </div>
<br> <!-- 24-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/skyopt_before.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2006.10172">
                    <papertitle>Sky Optimization: Semantically Aware Image Processing of Skies in Low-Light Photography</papertitle>
                  </a>
                  <br>
                  <a href="https://sites.google.com/corp/view/orly-liba/">Orly Liba</a>,
                  <a href="https://www.linkedin.com/in/longqicai/en-us">Longqi Cai</a>,
                  <a href="https://ai.google/research/people/105312/">Yun-Ta Tsai</a>,
                  <a href="https://research.google/people/EladEban/">Elad Eban</a>,
                  <a href="https://research.google/people/YairMovshovitzAttias/">Yair Movshovitz-Attias</a>,
                  <a href="https://scholar.google.com/citations?user=2jXxOYQAAAAJ">Yael Pritch</a>,
                  <a href="https://www.linkedin.com/in/huizhong-chen-00776432">Huizhong Chen</a>,
                  <strong>Jonathan T. Barron</strong>
                  <br>
                  <em>NTIRE CVPRW</em>, 2020  
                  <br>
                  <a href="https://google.github.io/sky-optimization/">project page</a>
                  <p></p>
                  <p>If you want to photograph the sky, it helps to know where the sky is.</p>
                </div>
        </div>
    </div>
<br> <!-- 25-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/nightsight_after.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/1910.11336">
                    <papertitle>Handheld Mobile Photography in Very Low Light</papertitle>
                  </a>
                  <br>
                  <a href="https://sites.google.com/site/orlylibaprofessional/">Orly Liba</a>,
                  <a href="https://scholar.google.com/citations?user=6PhlPWMAAAAJ">Kiran Murthy</a>,
                  <a href="https://ai.google/research/people/105312/">Yun-Ta Tsai</a>,
                  <a href="https://www.timothybrooks.com/">Timothy Brooks</a>,
                  <a href="https://people.csail.mit.edu/tfxue/">Tianfan Xue</a>,
                  <a href="https://scholar.google.com/citations?user=qgc_jY0AAAAJ">Nikhil Karnad</a>,
                  <a href="https://scholar.google.com/citations?user=BxqV_RsAAAAJ">Qiurui He</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="https://ai.google/research/people/105641/">Dillon Sharlet</a>,
                  <a href="http://www.geisswerks.com/">Ryan Geiss</a>,
                  <a href="https://people.csail.mit.edu/hasinoff/">Samuel W. Hasinoff</a>,
                  <a href="https://scholar.google.com/citations?user=2jXxOYQAAAAJ">Yael Pritch</a>,
                  <a href="http://graphics.stanford.edu/~levoy/">Marc Levoy</a>
                  <br>
                  <em>SIGGRAPH Asia</em>, 2019
                  <br>
                  <a href="https://github.com/google/night-sight/tree/master/docs">project page</a>
                  <br>
                  <p></p>
                  <p>By rethinking metering, white balance, and tone mapping, we can take pictures in places too dark for humans to see clearly.</p>
                </div>
        </div>
    </div>
<br> <!-- 26-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/font_before.png" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://www.matthewtancik.com/learnit">
                    <papertitle>Learned Initializations for Optimizing Coordinate-Based Neural Representations</papertitle>
                  </a>
                  <br>
                  <a href="https://arxiv.org/abs/1910.00748">
                    <papertitle>A Deep Factorization of Style and Structure in Fonts</papertitle>
                  </a>
                  <br>
                  <a href="http://www.cs.cmu.edu/~asrivats/">Nikita Srivatsan</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="https://people.eecs.berkeley.edu/~klein/">Dan Klein</a>,
                  <a href="http://cseweb.ucsd.edu/~tberg/">Taylor Berg-Kirkpatrick</a>
                  <br>
                  <em>EMNLP</em>, 2019 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                  <br>
                  <p></p>
                  <p>Variational auto-encoders can be used to disentangle a characters style from its content.</p>
                </div>
        </div>
    </div>
<br> <!-- 27-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/dpzlearn_after.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/1904.05822">
                    <papertitle>Learning Single Camera Depth Estimation using Dual-Pixels</papertitle>
                  </a>
                  <br>
                  <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>,
                  <a href="http://nealwadhwa.com">Neal Wadhwa</a>,
                  <a href="">Sameer Ansari,</a>,
                  <strong>Jonathan T. Barron</strong>
                  <br>
                  <em>ICCV</em>, 2019 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                  <br>
                  <a href="https://github.com/google-research/google-research/tree/master/dual_pixels">code</a> /
                  <a href="data/GargICCV2019.bib">bibtex</a>
                  <p></p>
                  <p>Considering the optics of dual-pixel image sensors improves monocular depth estimation techniques.</p>
                </div>
        </div>
    </div>
<br> <!-- 28-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/porlight_before.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://cseweb.ucsd.edu/~viscomp/projects/SIG19PortraitRelighting/">
                    <papertitle>Single Image Portrait Relighting</papertitle>
                  </a>
                  <br>
                  <a href="http://kevinkingo.com/">Tiancheng Sun</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="https://ai.google/research/people/105312/">Yun-Ta Tsai</a>,
                  <a href="https://cseweb.ucsd.edu/~zex014/">Zexiang Xu</a>, Xueming Yu,
                  <a href="http://ict.usc.edu/profile/graham-fyffe/">Graham Fyffe</a>, Christoph Rhemann, Jay Busch,
                  <a href="https://www.pauldebevec.com/">Paul Debevec</a>,
                  <a href="https://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>
                  <br>
                  <em>SIGGRAPH</em>, 2019
                  <br>
                  <a href="http://cseweb.ucsd.edu/~viscomp/projects/SIG19PortraitRelighting/">project page</a> / 
                  <a href="https://arxiv.org/abs/1905.00824">arxiv</a> / 
                  <a href="https://www.youtube.com/watch?v=yxhGWds_g4I">video</a> /
                  <a href="https://petapixel.com/2019/07/16/researchers-developed-an-ai-that-can-relight-portraits-after-the-fact/">press</a> /
                  <a href="data/SunSIGGRAPH2019.bib">bibtex</a>
                  <br>
                  <p></p>
                  <p>Training a neural network on light stage scans and environment maps produces an effective relighting method.</p>
               </div>
        </div>
    </div>
<br> <!-- 29-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/loss_after.png" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/open?id=1xpZ0fL9h1y9RfcTyPgVkxUrF3VwdkBvq">
                    <papertitle>A General and Adaptive Robust Loss Function</papertitle>
                  </a>
                  <br>
                  <strong>Jonathan T. Barron</strong>
                  <br>
                  <em>CVPR</em>, 2019 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Award Finalist)</strong></font>
                  <br>
                  <a href="https://arxiv.org/abs/1701.03077">arxiv</a> /
                  <a href="https://drive.google.com/open?id=1HNveL7xSNh6Ss7sxLK8Mw2L1Fc-rRhL4">supplement</a> /
                  <a href="https://youtu.be/BmNKbnF69eY">video</a> /
                  <a href="https://www.youtube.com/watch?v=4IInDT_S0ow&t=37m22s">talk</a> / 
                  <a href="https://drive.google.com/file/d/1GzRYRIfLHvNLT_QwjHoBjHkBbs3Nbf0x/view?usp=sharing">slides</a> / 
                  code: <a href="https://github.com/google-research/google-research/tree/master/robust_loss">TF</a>, <a href="https://github.com/google-research/google-research/tree/master/robust_loss_jax">JAX</a>, <a href="https://github.com/jonbarron/robust_loss_pytorch">pytorch</a> /
                  <a href="data/BarronCVPR2019_reviews.txt">reviews</a> /
                  <a href="data/BarronCVPR2019.bib">bibtex</a>
                  <p></p>
                  <p>A single robust loss function is a superset of many other common robust loss functions, and allows training to automatically adapt the robustness of its own loss.</p>
                </div>
        </div>
    </div>
<br> <!-- 30-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/mpi_before.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1TU5L6fnt4Kd49IUOU7aNxor5NIgdHuNG/view?usp=sharing">
                    <papertitle>Pushing the Boundaries of View Extrapolation with Multiplane Images</papertitle>
                  </a>
                  <br>
                  <a href="https://pratulsrinivasan.github.io/">Pratul P. Srinivasan</a>, Richard Tucker,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
                  <a href="http://graphics.stanford.edu/~renng/">Ren Ng</a>,
                  <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>
                  <br>
                  <em>CVPR</em>, 2019 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Award Finalist)</strong></font>
                  <br>
                  <a href="https://drive.google.com/file/d/1GUW_n-BAn9Q4VntEA_OTHNJiHO7XfC62/view?usp=sharing">supplement</a> /
                  <a href="https://www.youtube.com/watch?v=aJqAaMNL2m4">video</a> /
                  <a href="data/SrinivasanCVPR2019.bib">bibtex</a>
                  <p></p>
                  <p>View extrapolation with multiplane images works better if you reason about disocclusions and disparity sampling frequencies.</p>
                 </div>
        </div>
    </div>
<br> <!-- 31-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/unprocessing_before.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1H0Wtd--un2JN76dUJN8iC9fWfkA16n8D/view?usp=sharing">
                    <papertitle>Unprocessing Images for Learned Raw Denoising</papertitle>
                  </a>
                  <br>
                  <a href="http://timothybrooks.com/">Tim Brooks</a>,
                  <a href="https://bmild.github.io/">Ben Mildenhall</a>,
                  <a href="https://people.csail.mit.edu/tfxue/">Tianfan Xue</a>,
                  <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>,
                  <a href="http://www.dsharlet.com/">Dillon Sharlet</a>,
                  <strong>Jonathan T. Barron</strong>
                  <br>
                  <em>CVPR</em>, 2019 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                  <br>
                  <a href="https://arxiv.org/abs/1811.11127">arxiv</a> /
                  <a href="http://timothybrooks.com/tech/unprocessing/">project page</a> /
                  <a href="https://github.com/google-research/google-research/tree/master/unprocessing">code</a> / 
                  <a href="data/BrooksCVPR2019.bib">bibtex</a>
                  <p></p>
                  <p>We can learn a better denoising model by processing and unprocessing images the same way a camera does.</p>
                </div>
        </div>
    </div>
<br> <!-- 32-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/motionblur_before.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1hWpA4f6iLVcOkZI3zEAAWKARSQhnVgbY/view?usp=sharing">
                    <papertitle>Learning to Synthesize Motion Blur</papertitle>
                  </a>
                  <br>
                  <a href="http://timothybrooks.com/">Tim Brooks</a>,
                  <strong>Jonathan T. Barron</strong>
                  <br>
                  <em>CVPR</em>, 2019 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                  <br>
                  <a href="https://arxiv.org/abs/1811.11745">arxiv</a> /
                  <a href="https://drive.google.com/file/d/1dUQwBMmQdYYIP0zHR_nDQY-uQbaMdcSN/view?usp=sharing">supplement</a> /
                  <a href="http://timothybrooks.com/tech/motion-blur/">project page</a> /
                  <a href="https://www.youtube.com/watch?v=8T1jjSz-2V8">video</a> /
                  <a href="https://github.com/google-research/google-research/tree/master/motion_blur">code</a> / 
                  <a href="data/BrooksBarronCVPR2019.bib">bibtex</a>
                  <p></p>
                  <p>Frame interpolation techniques can be used to train a network that directly synthesizes linear blur kernels.</p>
                 </div>
        </div>
    </div>
<br> <!-- 33-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/darkflash_before.png" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/1901.01370">
                    <papertitle>Stereoscopic Dark Flash for Low-light Photography</papertitle>
                  </a>
                  <br>
                  <a href="https://www.andrew.cmu.edu/user/jianwan2/">Jian Wang</a>,
                  <a href="https://people.csail.mit.edu/tfxue/">Tianfan Xue</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>
                  <br>
                  <em>ICCP</em>, 2019
                  <br>
                  <p></p>
                  <p>
                    By making one camera in a stereo pair hyperspectral we can multiplex dark flash pairs in space instead of time.
                  </p></div>
        </div>
    </div>
<br> <!-- 34-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/motionstereo_before.png" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1AABFJ3NgD5DAo5JEpEjWZrcQNzjZnvW9/view?usp=sharing">
                    <papertitle>Depth from Motion for Smartphone AR</papertitle>
                  </a>
                  <br>
                  <a href="https://www.linkedin.com/in/valentinjulien/">Julien Valentin</a>,
                  <a href="https://www.linkedin.com/in/adarshkowdle/">Adarsh Kowdle</a>,
                  <strong>Jonathan T. Barron</strong>, <a href="http://nealwadhwa.com">Neal Wadhwa</a>, and others
                  <br>
                  <em>SIGGRAPH Asia</em>, 2018
                  <br>
                  <a href="https://github.com/jonbarron/planar_filter">planar filter toy code</a> / 
                  <a href="data/Valentin2018.bib">bibtex</a>
                  <p></p>
                  <p>Depth cues from camera motion allow for real-time occlusion effects in augmented reality applications.</p>
                </div>
        </div>
    </div>
<br> <!-- 35-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/portrait_before.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/13i6DlS9UhGVKmwslLUFnKBwdxFRVQeQj/view?usp=sharing">
                    <papertitle>Synthetic Depth-of-Field with a Single-Camera Mobile Phone</papertitle>
                  </a>
                  <br>
                  <a href="http://nealwadhwa.com">Neal Wadhwa</a>,
                  <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>,
                  <a href="http://graphics.stanford.edu/~dejacobs/">David E. Jacobs</a>, Bryan E. Feldman, Nori Kanazawa, Robert Carroll,
                  <a href="http://www.cs.cmu.edu/~ymovshov/">Yair Movshovitz-Attias</a>,
                  <strong>Jonathan T. Barron</strong>, Yael Pritch,
                  <a href="http://graphics.stanford.edu/~levoy/">Marc Levoy</a>
                  <br>
                  <em>SIGGRAPH</em>, 2018
                  <br>
                  <a href="https://arxiv.org/abs/1806.04171">arxiv</a> /
                  <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">blog post</a> /
                  <a href="data/Wadhwa2018.bib">bibtex</a>
                  <p></p>
                  <p>Dual pixel cameras and semantic segmentation algorithms can be used for shallow depth of field effects.</p>
                  <p>This system is the basis for "Portrait Mode" on the Google Pixel 2 smartphones</p>
                </div>
        </div>
    </div>
<br> <!-- 36-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/aperture_before.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1MpvxcW7OTJP321QL_q4ZLQ8D653bZZzy/view?usp=sharing">
                    <papertitle>Aperture Supervision for Monocular Depth Estimation</papertitle>
                  </a>
                  <br>
                  <a href="https://pratulsrinivasan.github.io/">Pratul P. Srinivasan</a>,
                  <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>,
                  <a href="http://nealwadhwa.com">Neal Wadhwa</a>,
                  <a href="http://graphics.stanford.edu/~renng/">Ren Ng</a>,
                  <strong>Jonathan T. Barron</strong>
                  <br>
                  <em>CVPR</em>, 2018
                  <br>
                  <a href="https://github.com/google/aperture_supervision">code</a> /
                  <a href="data/Srinivasan2018.bib">bibtex</a>
                  <p></p>
                  <p>Varying a camera's aperture provides a supervisory signal that can teach a neural network to do monocular depth estimation.</p>
                </div>
        </div>
    </div>
<br> <!-- 37-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/deepburst_before.png" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1GAH8ijyZ7GnoBnQFANEzdXinHrE4vvXn/view?usp=sharing">
                    <papertitle>Burst Denoising with Kernel Prediction Networks</papertitle>
                  </a>
                  <br>
                  <a href="https://bmild.github.io/">Ben Mildenhall</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>,
                  <a href="http://www.dsharlet.com/">Dillon Sharlet</a>,
                  <a href="http://graphics.stanford.edu/~renng/">Ren Ng</a>, Robert Carroll
                  <br>
                  <em>CVPR</em>, 2018 &nbsp <font color=#FF8080><strong>(Spotlight)</strong></font>
                  <br>
                  <a href="https://drive.google.com/file/d/1aqk3Q-L2spjLZh2yRWKUWIDcZkGjQ7US/view?usp=sharing">supplement</a> /
                  <a href="https://github.com/google/burst-denoising">code</a> /
                  <a href="data/Mildenhall2018.bib">bibtex</a>
                  <p></p>
                  <p>We train a network to predict linear kernels that denoise noisy bursts from cellphone cameras.</p>
                </div>
        </div>
    </div>
<br> <!-- 38-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/friendly_after.png" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1w_0djhL0QgC_fbehnJ0c-J23_kW_420p/view?usp=sharing">
                    <papertitle>A Hardware-Friendly Bilateral Solver for Real-Time Virtual Reality Video</papertitle>
                  </a>
                  <br>
                  <a href="https://homes.cs.washington.edu/~amrita/">Amrita Mazumdar</a>, <a href="http://homes.cs.washington.edu/~armin/">Armin Alaghi</a>, <strong>Jonathan T. Barron</strong>, <a href="https://www.cs.unc.edu/~gallup/">David Gallup</a>, <a href="https://homes.cs.washington.edu/~luisceze/">Luis Ceze</a>, <a href="https://homes.cs.washington.edu/~oskin/">Mark Oskin</a>, <a href="http://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a>
                  <br>
                  <em>High-Performance Graphics (HPG)</em>, 2017
                  <br>
                  <a href="https://sampa.cs.washington.edu/projects/vr-hw.html">project page</a>
                  <p></p>
                  <p>A reformulation of the bilateral solver can be implemented efficiently on GPUs and FPGAs.</p>
                </div>
        </div>
    </div>
<br> <!-- 39-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/hdrnet_after.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1jQY3CTMnLX7PeGUzYLso9H1eCsZyWbwg/view?usp=sharing">
                    <papertitle>Deep Bilateral Learning for Real-Time Image Enhancement</papertitle>
                  </a>
                  <br>
                  <a href="http://www.mgharbi.com">Micha&euml;l Gharbi</a>, <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>, <strong>Jonathan T. Barron</strong>, <a href="https://people.csail.mit.edu/hasinoff/">Samuel W. Hasinoff</a>, <a href="http://people.csail.mit.edu/fredo/">Fr&eacute;do Durand </a>
                  <br>
                  <em>SIGGRAPH</em>, 2017
                  <br>
                  <a href="https://groups.csail.mit.edu/graphics/hdrnet/">project page</a> /
                  <a href="https://www.youtube.com/watch?v=GAe0qKKQY_I">video</a> /
                  <a href="data/GharbiSIGGRAPH2017.bib">bibtex</a> /
                  <a href="http://news.mit.edu/2017/automatic-image-retouching-phone-0802">p</a><a href="https://www.wired.com/story/googles-new-algorithm-perfects-photos-before-you-even-take-them/">r</a><a href="https://petapixel.com/2017/08/02/new-ai-can-retouch-photos-snap/">e</a><a href="https://www.theverge.com/2017/8/2/16082272/google-mit-retouch-photos-machine-learning">s</a><a href="http://gizmodo.com/clever-camera-app-uses-deep-learning-to-perfectly-retou-1797474282">s</a>
                  <p></p>
                  <p>By training a deep network in bilateral space we can learn a model for high-resolution and real-time image enhancement.</p>
                </div>
        </div>
    </div>
<br> <!-- 40-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/ffcc_after.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/1611.07596">
                    <papertitle>Fast Fourier Color Constancy</papertitle>
                  </a>
                  <br>
                  <strong>Jonathan T. Barron</strong>,
                  <a href="https://ai.google/research/people/105312/">Yun-Ta Tsai</a>,
                  <br>
                  <em>CVPR</em>, 2017
                  <br>
                  <a href="https://youtu.be/rZCXSfl13rY">video</a> /
                  <a href="data/BarronTsaiCVPR2017.bib">bibtex</a> /
                  <a href="https://github.com/google/ffcc">code</a> /
                  <a href="https://drive.google.com/open?id=0B4nuwEMaEsnmWkJQMlFPSFNzbEk">output</a> /
                  <a href="https://blog.google/products/photos/six-tips-make-your-photos-pop/">blog post</a> /
                  <a href="https://9to5google.com/2017/03/03/google-photos-auto-white-balance/">p</a><a href="https://www.engadget.com/2017/03/03/google-photos-automatically-fixes-your-pictures-white-balance/">r</a><a href="https://lifehacker.com/google-photos-will-now-automatically-adjust-the-white-b-1793009155">e</a><a href="https://petapixel.com/2017/03/06/google-photos-will-now-automatically-white-balance-snapshots/">s</a><a href="http://www.theverge.com/2017/3/3/14800062/google-photos-auto-white-balance-android">s</a>
                  <p></p>
                  <p>Color space can be aliased, allowing white balance models to be learned and evaluated in the frequency domain. This improves accuracy by 13-20% and speed by 250-3000x.</p>
                  <p>This technology is used by <a href="https://store.google.com/product/pixel_compare">Google Pixel</a>, <a href="https://photos.google.com/">Google Photos</a>, and <a href="https://www.google.com/maps">Google Maps</a>.</p>
               </div>
        </div>
    </div>
<br> <!-- 41-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/jump_still.png" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1RBnTrtzqmuO8uj3GQaR5vBJZjIC3Jxjn/view?usp=sharing">
                    <papertitle>Jump: Virtual Reality Video</papertitle>
                  </a>
                  <br>
                  <a href="http://mi.eng.cam.ac.uk/~ra312/">Robert Anderson</a>, <a href="https://www.cs.unc.edu/~gallup/">David Gallup</a>, <strong>Jonathan T. Barron</strong>, <a href="https://mediatech.aalto.fi/~janne/index.php">Janne Kontkanen</a>, <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>, <a href="http://carlos-hernandez.org/">Carlos Hern&aacutendez</a>, <a href="https://homes.cs.washington.edu/~sagarwal/">Sameer Agarwal</a>, <a href="https://homes.cs.washington.edu/~seitz/">Steven M Seitz</a>
                  <br>
                  <em>SIGGRAPH Asia</em>, 2016
                  <br>
                  <a href="https://drive.google.com/file/d/11D4eCDXqqFTtZT0WS2COJE0hsAN3QEww/view?usp=sharing">supplement</a> /
                  <a href="https://www.youtube.com/watch?v=O0qUYynupTI">video</a> /
                  <a href="data/Anderson2016.bib">bibtex</a> /
                  <a href="https://blog.google/products/google-vr/jump-using-omnidirectional-stereo-vr-video/">blog post</a>
                  <p></p>
                  <p>Using computer vision and a ring of cameras, we can make video for virtual reality headsets that is both stereo and 360&deg;.</p>
                  <p>This technology is used by <a href="https://vr.google.com/jump/">Jump</a>. </p>
                </div>
        </div>
    </div>
<br> <!-- 42-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/hdrp_after.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1SSSmVHWbMQ7sZMOredSVWVJXbXobkyzA/view?usp=sharing">
                    <papertitle>Burst Photography for High Dynamic Range and Low-Light Imaging on Mobile Cameras</papertitle>
                  </a>
                  <br>
                  <a href="http://people.csail.mit.edu/hasinoff/">Samuel W. Hasinoff</a>, <a href="http://www.dsharlet.com/">Dillon Sharlet</a>, <a href="http://www.geisswerks.com/">Ryan Geiss</a>, <a href="http://people.csail.mit.edu/abadams/">Andrew Adams</a>, <strong>Jonathan T. Barron</strong>, Florian Kainz, <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>, <a href="http://graphics.stanford.edu/~levoy/">Marc Levoy</a>
                  <br>
                  <em>SIGGRAPH Asia</em>, 2016
                  <br>
                  <a href="http://hdrplusdata.org/">project page</a> /
                  <a href="https://drive.google.com/open?id=15EUuSDi1BtHUgQCaiooVrD44qYKIC3vx">supplement</a> /
                  <a href="data/Hasinoff2016.bib">bibtex</a>
                  <p></p>
                  <p>Mobile phones can take beautiful photographs in low-light or high dynamic range environments by aligning and merging a burst of images.</p>
                  <p>This technology is used by the <a href="https://research.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">Nexus HDR+</a> feature.</p>
                 </div>
        </div>
    </div>
<br> <!-- 43-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/BS_after.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1zFzCaFwkGK1EGmJ_KEqb-ZsRJhfUKN2S/view?usp=sharing">
                    <papertitle>The Fast Bilateral Solver</papertitle>
                  </a>
                  <br>
                  <strong>Jonathan T. Barron</strong>,
                  <a href="https://cs.stanford.edu/~poole/">Ben Poole</a>
                  <br>
                  <em>ECCV</em>, 2016 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Honorable Mention)</strong></font>
                  <br>
                  <a href="http://arxiv.org/abs/1511.03296">arXiv</a> /
                  <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmdEREcjhlSXM2NGs/view?usp=sharing">supplement</a> /
                  <a href="data/BarronPooleECCV2016.bib">bibtex</a> /
                  <a href="http://videolectures.net/eccv2016_barron_bilateral_solver/">video (they messed up my slides, use &rarr;)</a> /
                  <a href="https://drive.google.com/file/d/19x1AeN0PFus6Pjrd8nR-vCmJ6bNEefsC/view?usp=sharing">keynote</a> (or <a href="https://drive.google.com/file/d/1p9nduiymK9jUh7WfwlsMjBfW8RoNe_61/view?usp=sharing">PDF</a>) /
                  <a href="https://github.com/poolio/bilateral_solver">code</a> /
                  <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmaDI3bm5VeDRxams/view?usp=sharing">depth super-res results</a> /
                  <a href="data/BarronPooleECCV2016_reviews.txt">reviews</a>
                  <p></p>
                  <p>Our solver smooths things better than other filters and faster than other optimization algorithms, and you can backprop through it.</p>
                </div>
        </div>
    </div>
<br> <!-- 44-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/diverdi_before.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1mmT-LuK_eBZsl3qp4-fAshEPdgfbgvNE/view?usp=sharing">
                    <papertitle>Geometric Calibration for Mobile, Stereo, Autofocus Cameras</papertitle>
                  </a>
                  <br>
                  <a href="http://www.stephendiverdi.com/">Stephen DiVerdi</a>,
                  <strong>Jonathan T. Barron</strong>
                  <br>
                  <em>WACV</em>, 2016
                  <br>
                  <a href="data/Diverdi2016.bib">bibtex</a>
                  <p></p>
                  <p>Standard techniques for stereo calibration don't work for cheap mobile cameras.</p>
                </div>
        </div>
    </div>
<br> <!-- 45-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/DT_image.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/178Xj2PZ1w6hZJpucU-TiZOoCemJmvsVQ/view?usp=sharing">
                    <papertitle>Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform</papertitle>
                  </a>
                  <br>
                  <em>CVPR</em>, 2016
                  <br>
                  <a href="http://liangchiehchen.com/">Liang-Chieh Chen</a>, <strong>Jonathan T. Barron</strong>, <a href="http://ttic.uchicago.edu/~gpapan/">George Papandreou</a>, <a href="http://www.cs.ubc.ca/~murphyk/">Kevin Murphy</a>, <a href="http://www.stat.ucla.edu/~yuille/">Alan L. Yuille</a>
                  <br>
                  <a href="data/Chen2016.bib">bibtex</a> /
                  <a href="http://liangchiehchen.com/projects/DeepLab.html">project page</a> /
                  <a href="https://bitbucket.org/aquariusjay/deeplab-public-ver2">code</a>
                  <p></p>
                  <p>By integrating an edge-aware filter into a convolutional neural network we can learn an edge-detector while improving semantic segmentation.</p>
                </div>
        </div>
    </div>
<br> <!-- 46-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/ccc_after.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1id74VNDL8ACrrWf6vYgN2M4kS8gd4n7w/view?usp=sharing">
                    <papertitle>Convolutional Color Constancy</papertitle>
                  </a>
                  <br>
                  <strong>Jonathan T. Barron</strong>
                  <br>
                  <em>ICCV</em>, 2015
                  <br>
                  <a href="https://drive.google.com/file/d/1vO3sVOMihmpNqsuASeR46Y_iME0lOANR/view?usp=sharing">supplement</a> / <a href="data/BarronICCV2015.bib">bibtex</a> / <a href="https://youtu.be/saHwKY9rfx0">video</a> (or <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmalBNUzlENUJSVDg/view?usp=sharing">mp4</a>)
                  <p></p>
                  <p>By framing white balance as a chroma localization task we can discriminatively learn a color constancy model that beats the state-of-the-art by 40%.</p>
                </div>
        </div>
    </div>
<br> <!-- 47-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/Shelhamer2015.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1stygV71uBruD7Ck9CaAQr7nREvr3DtUL/view?usp=sharing">
                    <papertitle>Scene Intrinsics and Depth from a Single Image</papertitle>
                  </a>
                  <br>
                  <a href="http://imaginarynumber.net/">Evan Shelhamer</a>, <strong>Jonathan T. Barron</strong>, <a href="http://www.eecs.berkeley.edu/%7Etrevor/">Trevor Darrell</a>
                  <br>
                  <em>ICCV Workshop</em>, 2015
                  <br>
                  <a href="data/Shelhamer2015.bib">bibtex</a>
                  <p></p>
                  <p>The monocular depth estimates produced by fully convolutional networks can be used to inform intrinsic image estimation.</p>
                </div>
        </div>
    </div>
<br> <!-- 48-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/BarronCVPR2015_still.gif" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1R4RdaBZIs-uJobhIFs9yKf3jIsaHQNH0/view?usp=sharing">
                    <papertitle>Fast Bilateral-Space Stereo for Synthetic Defocus</papertitle>
                  </a>
                  <br>
                  <strong>Jonathan T. Barron</strong>, <a href="http://people.csail.mit.edu/abadams/">Andrew Adams</a>, <a href="http://people.csail.mit.edu/yichangshih/">YiChang Shih</a>, <a href="http://carlos-hernandez.org/">Carlos Hern&aacutendez</a>
                  <br>
                  <em>CVPR</em>, 2015 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                  <br>
                  <a href="https://drive.google.com/file/d/125qgMdqeT1vojMIijIKcOF099LjUgUOL/view?usp=sharing">abstract</a> /
                  <a href="https://drive.google.com/file/d/1HGGvVOGxmPjvgdK5q3UD1Qb5Nttg6kq9/view?usp=sharing">supplement</a> /
                  <a href="data/BarronCVPR2015.bib">bibtex</a> /
                  <a href="http://techtalks.tv/talks/fast-bilateral-space-stereo-for-synthetic-defocus/61624/">talk</a> /
                  <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmSzZZdUJSMllSUkE/view?usp=sharing">keynote</a> (or <a href="https://drive.google.com/open?id=0B4nuwEMaEsnmZ1ZXUzBCWDJYeFU">PDF</a>)
                  <p></p>
                  <p>By embedding a stereo optimization problem in "bilateral-space" we can very quickly solve for an edge-aware depth map, letting us render beautiful depth-of-field effects.</p>
                  <p>This technology is used by the <a href="http://googleresearch.blogspot.com/2014/04/lens-blur-in-new-google-camera-app.html">Google Camera "Lens Blur"</a> feature. </p>
                </div>
        </div>
    </div>
<br> <!-- 49-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/PABMM2015.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/1503.00848" id="MCG_journal">
                    <papertitle>Multiscale Combinatorial Grouping for Image Segmentation and Object Proposal Generation</papertitle>
                  </a>
                  <br>
                  <a href="http://imatge.upc.edu/web/people/jordi-pont-tuset">Jordi Pont-Tuset</a>, <a href="http://www.cs.berkeley.edu/~arbelaez/">Pablo Arbel&aacuteez</a>, <strong>Jonathan T. Barron</strong>, <a href="http://imatge.upc.edu/web/ferran">Ferran Marqu&eacutes</a>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
                  <br>
                  <em>TPAMI</em>, 2017
                  <br>
                  <a href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/">project page</a> /
                  <a href="data/PontTusetTPAMI2017.bib">bibtex</a> /
                  <a href="https://drive.google.com/file/d/1AiB78Fy7QVA3KqgcooyzMAC5L8HhNzjz/view?usp=sharing">fast eigenvector code</a>
                  <p></p>
                  <p>We produce state-of-the-art contours, regions and object candidates, and we compute normalized-cuts eigenvectors 20&times faster.</p>
                  <p>This paper subsumes our CVPR 2014 paper.</p>
                </div>
        </div>
    </div>
<br> <!-- 50-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/Estee.jpeg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2010.03592" id="SIRFS">
                    <papertitle>Shape, Illumination, and Reflectance from Shading</papertitle>
                  </a>
                  <br>
                  <strong>Jonathan T. Barron</strong>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
                  <br>
                  <em>TPAMI</em>, 2015
                  <br>
                  <a href="data/BarronMalikTPAMI2015.bib">bibtex</a> / <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmVWpfa19mbUxIYW8/view?usp=sharing">keynote</a> (or <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmazJvLXJUb0NuM1U/view?usp=sharing">powerpoint</a>, <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmTDBUWE96VHJndjg/view?usp=sharing">PDF</a>) / <a href="http://www.youtube.com/watch?v=NnePYprvFvA">video</a> / <a href="https://drive.google.com/file/d/1vg9Rb-kBntSTnTCzVgFlskkPXvTB_5aq/view?usp=sharing">code &amp; data</a> / <a href="https://drive.google.com/file/d/11X5Zfjy7Q7oP_V2rtqy2f5-x9YgQUAFd/view?usp=sharing">kudos</a>
                </p>
                <p>
                  We present <strong>SIRFS</strong>, which can estimate shape, chromatic illumination, reflectance, and shading from a single image of an masked object.
                </p>
                <p>
                  This paper subsumes our CVPR 2011, CVPR 2012, and ECCV 2012 papers.
                </p></div>
        </div>
    </div>
<br> <!-- 51-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/ArbalaezCVPR2014.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1M0wijHY134F9ETBgO8mjeuKUSblTRLG0/view?usp=sharing">
                    <papertitle>Multiscale Combinatorial Grouping</papertitle>
                  </a>
                  <br>
                  <a href="http://www.cs.berkeley.edu/~arbelaez/">Pablo Arbel&aacuteez</a>, <a href="http://imatge.upc.edu/web/people/jordi-pont-tuset">Jordi Pont-Tuset</a>, <strong>Jonathan T. Barron</strong>, <a href="http://imatge.upc.edu/web/ferran">Ferran Marqu&eacutes</a>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
                  <br>
                  <em>CVPR</em>, 2014
                  <br>
                  <a href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/">project page</a> /
                  <a href="data/ArbelaezCVPR2014.bib">bibtex</a>
                  <p>This paper is subsumed by <a href="#MCG_journal">our journal paper</a>.</p>
                </div>
        </div>
    </div>
<br> <!-- 52-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/BarronICCV2013.gif" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1shvItvx_8Sb8QNXhrOXkuRmx2618iwNJ/view?usp=sharing">
                    <papertitle>Volumetric Semantic Segmentation using Pyramid Context Features</papertitle>
                  </a>
                  <br>
                  <strong>Jonathan T. Barron</strong>, <a href="http://www.cs.berkeley.edu/~arbelaez/">Pablo Arbel&aacuteez</a>, <a href="http://big.lbl.gov/">Soile V. E. Ker&aumlnen</a>, <a href="http://www.lbl.gov/gsd/biggin.html">Mark D. Biggin</a>,
                  <br> <a href="http://dwknowles.lbl.gov/">David W. Knowles</a>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
                  <br>
                  <em>ICCV</em>, 2013
                  <br>
                  <a href="https://drive.google.com/file/d/1htiLpMAcYLtuBthmAb4XHnOYxUbkfnqR/view?usp=sharing">supplement</a> /
                  <a href="https://drive.google.com/file/d/1qoYeFNa443myn2SfcdhmCsYBqE9xQrPD/view?usp=sharing">poster</a> /
                  <a href="data/BarronICCV2013.bib">bibtex</a> / <a href="http://www.youtube.com/watch?v=Y56-FcfnlVA&hd=1">video 1</a> (or <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmZ1ZLaHdQYzAxNlU/view?usp=sharing">mp4</a>) / <a href="http://www.youtube.com/watch?v=mvRoYuP6-l4&hd=1">video 2</a> (or <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmZ1ZLaHdQYzAxNlU/view?usp=sharing">mp4</a>) / <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmSF9YdWJjQmh4QW8/view?usp=sharing">code &amp; data</a>
                  <p>
                    We present a technique for efficient per-voxel linear classification, which enables accurate and fast semantic segmentation of volumetric Drosophila imagery.
                  </p></div>
        </div>
    </div>
<br> <!-- 53-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/3DSP_160.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmbG1tOGIta3N1Wjg/view?usp=sharing" id="3DSP">
                    <papertitle>3D Self-Portraits</papertitle>
                  </a>
                  <br>
                  <a href="http://www.hao-li.com/">Hao Li</a>, <a href="http://www.evouga.com/">Etienne Vouga</a>, Anton Gudym, <a href="http://www.cs.princeton.edu/~linjiel/">Linjie Luo</a>, <strong>Jonathan T. Barron</strong>, Gleb Gusev
                  <br>
                  <em>SIGGRAPH Asia</em>, 2013
                  <br>
                  <a href="http://www.youtube.com/watch?v=DmUkbZ0QMCA">video</a> / <a href="http://shapify.me/">shapify.me</a> / <a href="data/3DSP_siggraphAsia2013.bib">bibtex</a>
                  <p>Our system allows users to create textured 3D models of themselves in arbitrary poses using only a single 3D sensor.</p>
                </div>
        </div>
    </div>
<br> <!-- 54-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/SceneSIRFS.gif" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1snypSLhzC0jXCchJRsWpcDZ7Es5hDmXo/view?usp=sharing">
                    <papertitle>Intrinsic Scene Properties from a Single RGB-D Image</papertitle>
                  </a>
                  <br>
                  <strong>Jonathan T. Barron</strong>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
                  <br>
                  <em>CVPR</em>, 2013 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                  <br>
                  <a href="https://drive.google.com/file/d/1cLUw72WpgdZ_3TQAjJABdgywqjBfn_Mq/view?usp=sharing">supplement</a> / <a href="data/BarronMalikCVPR2013.bib">bibtex</a> / <a href="http://techtalks.tv/talks/intrinsic-scene-properties-from-a-single-rgb-d-image/58614/">talk</a> / <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmWW1CZGJPbi12R0k/view?usp=sharing">keynote</a> (or <a href="https://drive.google.com/file/d/19q3EFf6GIb4UFcCN2DVU2jVKpxRj5kxf/view?usp=sharing">powerpoint</a>, <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmMzQ4ZVp1SWdnVkk/view?usp=sharing">PDF</a>) / <a href="https://drive.google.com/open?id=1ZbPScVA6Efqd-ESvojl92sw8K-82Xxry">code &amp; data</a>
                  <p>By embedding mixtures of shapes &amp; lights into a soft segmentation of an image, and by leveraging the output of the Kinect, we can extend SIRFS to scenes.
                    <br>
                    <br>TPAMI Journal version: <a href="https://drive.google.com/file/d/1iQiUxZvjPPnb8rFCwXYesTgFSRk7mkAq/view?usp=sharing">version</a> / <a href="data/BarronMalikTPAMI2015B.bib">bibtex</a>
                  </p></div>
        </div>
    </div>
<br> <!-- 55-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/Boundary.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1H4YPovfrvcce3HGMEhidwU2l2fTcNR5y/view?usp=sharing">
                    <papertitle>Boundary Cues for 3D Object Shape Recovery</papertitle>
                  </a>
                  <br>
                  <a href="http://www.kevinkarsch.com/">Kevin Karsch</a>,
                  <a href="http://web.engr.illinois.edu/~liao17/">Zicheng Liao</a>,
                  <a href="http://web.engr.illinois.edu/~jjrock2/">Jason Rock</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="http://www.cs.illinois.edu/homes/dhoiem/">Derek Hoiem</a>
                  <br>
                  <em>CVPR</em>, 2013
                  <br>
                  <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmLUQ5SVJTcUZIYXc/view?usp=sharing">supplement</a> / <a href="data/KarschCVPR2013.bib">bibtex</a>
                  <p>Boundary cues (like occlusions and folds) can be used for shape reconstruction, which improves object recognition for humans and computers.</p>
                </div>
        </div>
    </div>
<br> <!-- 56-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/ECCV2012_small.gif" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1NczR4pJ-s0YBjCe0rCevMt8IM5JPuUrc/view?usp=sharing">
                    <papertitle>Color Constancy, Intrinsic Images, and Shape Estimation</papertitle>
                  </a>
                  <br>
                  <strong>Jonathan T. Barron</strong>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
                  <br>
                  <em>ECCV</em>, 2012
                  <br>
                  <a href="https://drive.google.com/file/d/1zuxhWZ3i6THvuRRBeE7dM_BJfDxO72Fq/view?usp=sharing">supplement</a> /
                  <a href="data/BarronMalikECCV2012.bib">bibtex</a> /
                  <a href="https://drive.google.com/file/d/12x8mhqpFsA6p0u6ZQW-ieRKF8hlQBKKe/view?usp=sharing">poster</a> /
                  <a href="http://www.youtube.com/watch?v=NnePYprvFvA">video</a>
                  <p>This paper is subsumed by <a href="#SIRFS">SIRFS</a>.</p>
                </div>
        </div>
    </div>
<br> <!-- 57-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/BarronCVPR2012.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/17RfINbE2dr2EjXp9MtGO0MHJLQmQVhvT/view?usp=sharing">
                    <papertitle>Shape, Albedo, and Illumination from a Single Image of an Unknown Object</papertitle>
                  </a>
                  <br>
                  <strong>Jonathan T. Barron</strong>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
                  <br>
                  <em>CVPR</em>, 2012
                  <br>
                  <a href="https://drive.google.com/file/d/1Im_bUI42AP9VPoNtsjLajvtLRiwv39k3/view?usp=sharing">supplement</a> /
                  <a href="data/BarronMalikCVPR2012.bib">bibtex</a> /
                  <a href="https://drive.google.com/file/d/1IAlSF4k3_CEL9dfbaMiNTFPBoEkLhsRl/view?usp=sharing">poster</a>
                  <p>This paper is subsumed by <a href="#SIRFS">SIRFS</a>.</p>
                </div>
        </div>
    </div>
<br> <!-- 58-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/B3DO.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1_S8EQyngbHQrB415o0XkQ4V9SMzdEgWT/view?usp=sharing">
                    <papertitle>A Category-Level 3-D Object Dataset: Putting the Kinect to Work</papertitle>
                  </a>
                  <br>
                  <a href="http://www.eecs.berkeley.edu/%7Eallie/">Allison Janoch</a>,
                  <a href="http://sergeykarayev.com/">Sergey Karayev</a>,
                  <a href="http://www.eecs.berkeley.edu/%7Ejiayq/">Yangqing Jia</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="http://www.cs.berkeley.edu/%7Emfritz/">Mario Fritz</a>,
                  <a href="http://www.icsi.berkeley.edu/%7Esaenko/">Kate Saenko</a>,
                  <a href="http://www.eecs.berkeley.edu/%7Etrevor/">Trevor Darrell</a>
                  <br>
                  <em>ICCV 3DRR Workshop</em>, 2011
                  <br>
                  <a href="data/B3DO_ICCV_2011.bib">bibtex</a> /
                  <a href="https://drive.google.com/file/d/1qf4-U5RhSw12O7gzQwW66SMQhs2FWYDW/view?usp=sharing">"smoothing" code</a>
                  <p>We present a large RGB-D dataset of indoor scenes and investigate ways to improve object detection using depth information.</p>
                </div>
        </div>
    </div>
<br> <!-- 59-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/safs_160.gif" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1EZTOO5xezLYcyIFgAzs4KuZFLbTcwTDH/view?usp=sharing">
                    <papertitle>High-Frequency Shape and Albedo from Shading using Natural Image Statistics</papertitle>
                  </a>
                  <br>
                  <strong>Jonathan T. Barron</strong>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
                  <br>
                  <em>CVPR</em>, 2011
                  <br>
                  <a href="data/BarronMalikCVPR2011.bib">bibtex</a>
                  <p>This paper is subsumed by <a href="#SIRFS">SIRFS</a>.</p>
                </div>
        </div>
    </div>
<br> <!-- 60-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/fast_texture.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1rc05NatkQVmUDlGCAYcHSrvAzTpU9knT/view?usp=sharing">
                    <papertitle>Discovering Efficiency in Coarse-To-Fine Texture Classification</papertitle>
                  </a>
                  <br>
                  <strong>Jonathan T. Barron</strong>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
                  <br>
                  <em>Technical Report</em>, 2010
                  <br>
                  <a href="data/BarronTR2010.bib">bibtex</a>
                  <p>A model and feature representation that allows for sub-linear coarse-to-fine semantic segmentation.
                  </p></div>
        </div>
    </div>
<br> 
<!-- 61-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/prl.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/13rVuJpcytRdLYCnKpq46g7B7IzSrPQ2P/view?usp=sharing">
                    <papertitle>Parallelizing Reinforcement Learning</papertitle>
                  </a>
                  <br>
                  <strong>Jonathan T. Barron</strong>, <a href="http://www.eecs.berkeley.edu/~dsg/">Dave Golland</a>, <a href="http://www.cs.berkeley.edu/~nickjhay/">Nicholas J. Hay</a>
                  <br>
                  <em>Technical Report</em>, 2009
                  <br>
                  <a href="data/BarronPRL2009.bib">bibtex</a>
                  <p>Markov Decision Problems which lie in a low-dimensional latent space can be decomposed, allowing modified RL algorithms to run orders of magnitude faster in parallel.</p>
                </div>
        </div>
    </div>
<br> <!-- 62-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/bd_promo.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1PQjzKgFcrAesMIDJr-WDlCwuGUxZJZwO/view?usp=sharing">
                    <papertitle>Blind Date: Using Proper Motions to Determine the Ages of Historical Images</papertitle>
                  </a>
                  <br>
                  <strong>Jonathan T. Barron</strong>, <a href="http://cosmo.nyu.edu/hogg/">David W. Hogg</a>, <a href="http://www.astro.princeton.edu/~dstn/">Dustin Lang</a>, <a href="http://cs.nyu.edu/~roweis/">Sam Roweis</a>
                  <br>
                  <em>The Astronomical Journal</em>, 136, 2008
                  <p>Using the relative motions of stars we can accurately estimate the date of origin of historical astronomical images.</p>
                </div>
        </div>
    </div>
<br> 
<!-- 63-->   
<div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/clean_promo.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1YvRx-4hrZoCk-nl6OgVJZlHAqOiN5hWq/view?usp=sharing">
                    <papertitle>Cleaning the USNO-B Catalog Through Automatic Detection of Optical Artifacts</papertitle>
                  </a>
                  <br>
                  <strong>Jonathan T. Barron</strong>, <a href="http://stumm.ca/">Christopher Stumm</a>, <a href="http://cosmo.nyu.edu/hogg/">David W. Hogg</a>, <a href="http://www.astro.princeton.edu/~dstn/">Dustin Lang</a>, <a href="http://cs.nyu.edu/~roweis/">Sam Roweis</a>
                  <br>
                  <em>The Astronomical Journal</em>, 135, 2008
                  <p>We use computer vision techniques to identify and remove diffraction spikes and reflection halos in the USNO-B Catalog.</p>
                  <p>In use at <a href="http://www.astrometry.net">Astrometry.net</a></p>
                </div>
        </div>
    </div>
<br> 
</div></section>
            <hr class="m-0" />
            <!-- Service-->
            <section class="resume-section" id="education">
                <div class="resume-section-content">
                    <h2 class="mb-5">Service</h2>
                    <div  class="container-fluid">
    <div class="row">
        <div class="col-md-4">
            <div>
                <div class="one">
                  
                  <img src="assets/img/cvf.jpg" width="160">
                </div>
              </div>
        </div>
        <div class="col-md-8">
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair, Longuet-Higgins Award Committee Member, CVPR 2021</a>
              <br><br>
              <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
              <br><br>
              <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
             </div>
        </div>
    </div>
<br><br>
    
<div class="row">
    <div class="col-md-4">
        <div>
            <div class="one">
              
              <img src="assets/img/cs188.jpg" width="160">
            </div>
          </div>
    </div>
    <div class="col-md-8">
        <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
            <br>
            <br>
            <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
            <br>
            <br>
            <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
          </div>
    </div>
</div>
<br>
            </section>
            
        </div>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
